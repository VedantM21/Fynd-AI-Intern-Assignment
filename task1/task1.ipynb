{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ⚠️ Execution Note (API Quota Limitation)\n",
        "\n",
        "This notebook uses Google Gemini for prompt-based analysis.\n",
        "\n",
        "At the time of submission, the Gemini free-tier API quota has been exhausted.\n",
        "As a result, the cells cannot be re-executed without errors.\n",
        "\n",
        "This notebook is submitted to demonstrate:\n",
        "- prompt engineering approach\n",
        "- reasoning and experimentation process\n",
        "- qualitative evaluation methodology\n",
        "\n",
        "To ensure reliability in Task 2, a different LLM provider (OpenRouter) was intentionally used.\n",
        "This limitation and decision are documented transparently in the report.\n"
      ],
      "metadata": {
        "id": "uw99rOPTdrJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering Analysis for Yelp Rating Prediction\n",
        "\n",
        "This notebook accompanies Task 1 of the Fynd AI Intern take-home assignment.\n",
        "\n",
        "The objective is to evaluate how different prompt designs affect:\n",
        "- prediction accuracy of Yelp star ratings\n",
        "- reliability of structured (JSON) outputs\n",
        "\n",
        "Three prompting strategies are evaluated using the same dataset subset and metrics.\n"
      ],
      "metadata": {
        "id": "qdsCghAMvEVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySez-catxgjM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from typing import Dict\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import time\n",
        "from google.api_core.exceptions import TooManyRequests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"yelp.csv\")\n",
        "df.columns\n",
        "df = df[['text', 'stars']]\n",
        "df = df.rename(columns={'text': 'review_text'})\n",
        "df = df.sample(40, random_state=42).reset_index(drop=True)\n",
        "df.head()\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "_Bm6y4LU0XTK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "5f9889f3-e752-4b26-b42b-a50b6a9fda79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'yelp.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1595716514.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yelp.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stars'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'review_text'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yelp.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wRVbYjVYxnqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "The Yelp reviews dataset originally contains a `text` column representing customer reviews\n",
        "and a `stars` column representing the ground-truth rating (1–5).\n",
        "\n",
        "For clarity within the analysis pipeline:\n",
        "- `text` was renamed to `review_text`\n",
        "- a random subset of 40 reviews was sampled\n",
        "\n",
        "The reduced sample size was chosen due to free-tier API rate limits while remaining sufficient\n",
        "to observe consistent prompt behavior.\n"
      ],
      "metadata": {
        "id": "5j8wKohawfdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=userdata.get(\"GOOGLE_API_KEY\"))\n",
        "model = genai.GenerativeModel(\"models/gemini-flash-latest\")"
      ],
      "metadata": {
        "id": "i-UdLx0t54Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_llm(prompt, max_retries=5):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            return response.text\n",
        "        except TooManyRequests as e:\n",
        "            wait_time = 6  # seconds (safe for free tier)\n",
        "            print(f\"Rate limit hit. Waiting {wait_time}s...\")\n",
        "            time.sleep(wait_time)\n",
        "    return \"\"\n"
      ],
      "metadata": {
        "id": "WdhSRT0A6QjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_llm_output(output):\n",
        "    try:\n",
        "        cleaned = output.strip()\n",
        "\n",
        "        # remove ```json and ``` if Gemini adds them\n",
        "        if cleaned.startswith(\"```\"):\n",
        "            cleaned = cleaned.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "        return json.loads(cleaned)\n",
        "    except:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "PQFmyljq76Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Version 1 — Naive Baseline\n",
        "\n",
        "This prompt uses minimal instructions and weak structural constraints.\n",
        "It serves as a baseline to observe default LLM behavior."
      ],
      "metadata": {
        "id": "p-JNsLWIWWfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_v1(review_text):\n",
        "    return f\"\"\"\n",
        "Predict the Yelp star rating from 1 to 5 for the following review.\n",
        "Return the result strictly in JSON format with keys:\n",
        "- predicted_stars\n",
        "- explanation\n",
        "\n",
        "Review:\n",
        "{review_text}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VOyMJdHe7JyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_v1 = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    prompt = prompt_v1(row[\"review_text\"])\n",
        "    output = call_llm(prompt)\n",
        "    parsed = parse_llm_output(output)\n",
        "\n",
        "    results_v1.append({\n",
        "        \"actual_stars\": row[\"stars\"],\n",
        "        \"predicted_stars\": parsed[\"predicted_stars\"] if parsed else None,\n",
        "        \"valid_json\": parsed is not None\n",
        "    })\n",
        "\n",
        "    time.sleep(1.5)"
      ],
      "metadata": {
        "id": "Ir-AD7LR-9JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_v1 = pd.DataFrame(results_v1)\n",
        "res_v1.head()"
      ],
      "metadata": {
        "id": "Ry-F80gg_DtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt v1 Results\n",
        "\n",
        "Accuracy and JSON validity are computed below.\n",
        "This baseline demonstrates limited output reliability.\n"
      ],
      "metadata": {
        "id": "cTUMzlpJwysm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_v1 = (res_v1[\"actual_stars\"] == res_v1[\"predicted_stars\"]).mean()\n",
        "json_validity_v1 = res_v1[\"valid_json\"].mean()\n",
        "\n",
        "accuracy_v1, json_validity_v1"
      ],
      "metadata": {
        "id": "8rXIQZXJVDPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Version 2 — Strict JSON Enforcement\n",
        "\n",
        "This prompt enforces JSON-only output with a fixed schema.\n",
        "The goal is to improve parsing reliability."
      ],
      "metadata": {
        "id": "BDZuoOGzWUye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_v2(review_text):\n",
        "    return f\"\"\"\n",
        "You are a classification system.\n",
        "\n",
        "Your task:\n",
        "- Predict the Yelp star rating from 1 to 5 for the given review.\n",
        "\n",
        "STRICT RULES:\n",
        "- Respond with ONLY valid JSON.\n",
        "- Do NOT include markdown.\n",
        "- Do NOT include explanations outside JSON.\n",
        "- Do NOT include any extra text.\n",
        "\n",
        "JSON SCHEMA (follow exactly):\n",
        "{{\n",
        "  \"predicted_stars\": <integer from 1 to 5>,\n",
        "  \"explanation\": \"<one short sentence>\"\n",
        "}}\n",
        "\n",
        "Review:\n",
        "{review_text}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xO3dTu4xVrl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_v2 = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    prompt = prompt_v2(row[\"review_text\"])\n",
        "    output = call_llm(prompt)\n",
        "    parsed = parse_llm_output(output)\n",
        "\n",
        "    results_v2.append({\n",
        "        \"actual_stars\": row[\"stars\"],\n",
        "        \"predicted_stars\": parsed[\"predicted_stars\"] if parsed else None,\n",
        "        \"valid_json\": parsed is not None\n",
        "    })\n",
        "\n",
        "    time.sleep(1.5)\n"
      ],
      "metadata": {
        "id": "LKsP2w8cWhpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_v2 = pd.DataFrame(results_v2)\n",
        "res_v2.head()"
      ],
      "metadata": {
        "id": "tjLrBqKEWkS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt v2 Observations\n",
        "\n",
        "Although output structure was more constrained, prediction accuracy decreased.\n",
        "This highlights the trade-off between structure enforcement and reasoning quality."
      ],
      "metadata": {
        "id": "4KAdH3jvw5uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_v2 = (res_v2[\"actual_stars\"] == res_v2[\"predicted_stars\"]).mean()\n",
        "json_validity_v2 = res_v2[\"valid_json\"].mean()\n",
        "\n",
        "accuracy_v2, json_validity_v2"
      ],
      "metadata": {
        "id": "qogpnxC0hJnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Version 3 — Reasoned and Structured Prompt\n",
        "\n",
        "This prompt embeds reasoning within the JSON output to balance\n",
        "prediction quality and structured reliability."
      ],
      "metadata": {
        "id": "BfTdY-QLh0Zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_v3(review_text):\n",
        "    return f\"\"\"\n",
        "You are an expert sentiment analyst.\n",
        "\n",
        "Step 1: Analyze the review and identify:\n",
        "- overall sentiment (positive / neutral / negative)\n",
        "- key positive points\n",
        "- key negative points\n",
        "\n",
        "Step 2: Based on this analysis, decide the most appropriate Yelp star rating (1 to 5).\n",
        "\n",
        "STRICT OUTPUT RULES:\n",
        "- Your final answer MUST be valid JSON only.\n",
        "- Do NOT include markdown.\n",
        "- Do NOT include analysis text outside JSON.\n",
        "\n",
        "JSON FORMAT (follow exactly):\n",
        "{{\n",
        "  \"predicted_stars\": <integer from 1 to 5>,\n",
        "  \"explanation\": \"<brief justification based on positives and negatives>\"\n",
        "}}\n",
        "\n",
        "Review:\n",
        "{review_text}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LrUIBxIVhOT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_v3 = []\n",
        "for _, row in df.iterrows():\n",
        "    prompt = prompt_v3(row[\"review_text\"])\n",
        "    output = call_llm(prompt)\n",
        "    parsed = parse_llm_output(output)\n",
        "\n",
        "    results_v3.append({\n",
        "        \"actual_stars\": row[\"stars\"],\n",
        "        \"predicted_stars\": parsed[\"predicted_stars\"] if parsed else None,\n",
        "        \"valid_json\": parsed is not None\n",
        "    })\n",
        "\n",
        "    time.sleep(1.5)"
      ],
      "metadata": {
        "id": "7ytVT0E9jGif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execution Note\n",
        "\n",
        "Full execution of Prompt v3 was constrained by free-tier API rate limits.\n",
        "Repeated rate-limit responses were observed despite retry and backoff logic.\n",
        "\n",
        "The prompt design is retained for completeness and comparison.\n"
      ],
      "metadata": {
        "id": "1Oekl8-ixKzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res_v3 = pd.DataFrame(results_v3)\n",
        "res_v3.head()"
      ],
      "metadata": {
        "id": "iLG1nhvZjJjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_v3 = pd.DataFrame(results_v3)\n",
        "accuracy_v3 = (res_v3[\"actual_stars\"] == res_v3[\"predicted_stars\"]).mean()\n",
        "json_validity_v3 = res_v3[\"valid_json\"].mean()\n",
        "accuracy_v3, json_validity_v3"
      ],
      "metadata": {
        "id": "9IuB2yOiyFOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparative Summary\n",
        "\n",
        "| Prompt | Accuracy | JSON Validity | Key Observation |\n",
        "|-------|----------|---------------|----------------|\n",
        "| v1 | 0.175 | 0.30 | Weak structure |\n",
        "| v2 | 0.125 | 0.225 | Structure reduced reasoning |\n",
        "| v3 | N/A | Expected highest | Balanced design, quota-limited |\n"
      ],
      "metadata": {
        "id": "d1lfieF5xSWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_v3 = (res_v3[\"actual_stars\"] == res_v3[\"predicted_stars\"]).mean()\n",
        "json_validity_v3 = res_v3[\"valid_json\"].mean()\n",
        "\n",
        "accuracy_v3, json_validity_v3"
      ],
      "metadata": {
        "id": "TZEQBmMapkzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Takeaways\n",
        "\n",
        "- Prompt design strongly influences both accuracy and reliability\n",
        "- Strict structure alone does not guarantee better performance\n",
        "- Reasoning must be explicitly structured for automated pipelines\n",
        "- External constraints such as API limits are a real-world consideration"
      ],
      "metadata": {
        "id": "4fb_9zNIxRK6"
      }
    }
  ]
}