{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ⚠️ Execution Note (API Quota Limitation)\n",
        "\n",
        "This notebook uses Google Gemini for prompt-based analysis.\n",
        "\n",
        "At the time of submission, the Gemini free-tier API quota has been exhausted.\n",
        "As a result, the cells cannot be re-executed without errors.\n",
        "\n",
        "This notebook is submitted to demonstrate:\n",
        "- prompt engineering approach\n",
        "- reasoning and experimentation process\n",
        "- qualitative evaluation methodology\n",
        "\n",
        "To ensure reliability in Task 2, a different LLM provider (OpenRouter) was intentionally used.\n",
        "This limitation and decision are documented transparently in the report.\n"
      ],
      "metadata": {
        "id": "uw99rOPTdrJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering Analysis for Yelp Rating Prediction\n",
        "\n",
        "This notebook accompanies Task 1 of the Fynd AI Intern take-home assignment.\n",
        "\n",
        "The objective is to evaluate how different prompt designs affect:\n",
        "- prediction accuracy of Yelp star ratings\n",
        "- reliability of structured (JSON) outputs\n",
        "\n",
        "Three prompting strategies are evaluated using the same dataset subset and metrics.\n"
      ],
      "metadata": {
        "id": "qdsCghAMvEVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySez-catxgjM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from typing import Dict\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import time\n",
        "from google.api_core.exceptions import TooManyRequests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"yelp.csv\")\n",
        "df.columns\n",
        "df = df[['text', 'stars']]\n",
        "df = df.rename(columns={'text': 'review_text'})\n",
        "df = df.sample(40, random_state=42).reset_index(drop=True)\n",
        "df.head()\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "M1nQsbbYgLuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wRVbYjVYxnqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "The Yelp reviews dataset originally contains a `text` column representing customer reviews\n",
        "and a `stars` column representing the ground-truth rating (1–5).\n",
        "\n",
        "For clarity within the analysis pipeline:\n",
        "- `text` was renamed to `review_text`\n",
        "- a random subset of 40 reviews was sampled\n",
        "\n",
        "The reduced sample size was chosen due to free-tier API rate limits while remaining sufficient\n",
        "to observe consistent prompt behavior.\n"
      ],
      "metadata": {
        "id": "5j8wKohawfdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=userdata.get(\"GOOGLE_API_KEY\"))\n",
        "model = genai.GenerativeModel(\"models/gemini-flash-latest\")"
      ],
      "metadata": {
        "id": "i-UdLx0t54Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_llm(prompt, max_retries=5):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            return response.text\n",
        "        except TooManyRequests as e:\n",
        "            wait_time = 6  # seconds (safe for free tier)\n",
        "            print(f\"Rate limit hit. Waiting {wait_time}s...\")\n",
        "            time.sleep(wait_time)\n",
        "    return \"\"\n"
      ],
      "metadata": {
        "id": "WdhSRT0A6QjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_llm_output(output):\n",
        "    try:\n",
        "        cleaned = output.strip()\n",
        "\n",
        "        # remove ```json and ``` if Gemini adds them\n",
        "        if cleaned.startswith(\"```\"):\n",
        "            cleaned = cleaned.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "        return json.loads(cleaned)\n",
        "    except:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "PQFmyljq76Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Version 1 — Naive Baseline\n",
        "\n",
        "This prompt uses minimal instructions and weak structural constraints.\n",
        "It serves as a baseline to observe default LLM behavior."
      ],
      "metadata": {
        "id": "p-JNsLWIWWfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_v1(review_text):\n",
        "    return f\"\"\"\n",
        "Predict the Yelp star rating from 1 to 5 for the following review.\n",
        "Return the result strictly in JSON format with keys:\n",
        "- predicted_stars\n",
        "- explanation\n",
        "\n",
        "Review:\n",
        "{review_text}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VOyMJdHe7JyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_v1 = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    prompt = prompt_v1(row[\"review_text\"])\n",
        "    output = call_llm(prompt)\n",
        "    parsed = parse_llm_output(output)\n",
        "\n",
        "    results_v1.append({\n",
        "        \"actual_stars\": row[\"stars\"],\n",
        "        \"predicted_stars\": parsed[\"predicted_stars\"] if parsed else None,\n",
        "        \"valid_json\": parsed is not None\n",
        "    })\n",
        "\n",
        "    time.sleep(1.5)"
      ],
      "metadata": {
        "id": "Ir-AD7LR-9JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_v1 = pd.DataFrame(results_v1)\n",
        "res_v1.head()"
      ],
      "metadata": {
        "id": "Ry-F80gg_DtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt v1 Results\n",
        "\n",
        "Accuracy and JSON validity are computed below.\n",
        "This baseline demonstrates limited output reliability.\n"
      ],
      "metadata": {
        "id": "cTUMzlpJwysm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_v1 = (res_v1[\"actual_stars\"] == res_v1[\"predicted_stars\"]).mean()\n",
        "json_validity_v1 = res_v1[\"valid_json\"].mean()\n",
        "\n",
        "accuracy_v1, json_validity_v1"
      ],
      "metadata": {
        "id": "8rXIQZXJVDPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Version 2 — Strict JSON Enforcement\n",
        "\n",
        "This prompt enforces JSON-only output with a fixed schema.\n",
        "The goal is to improve parsing reliability."
      ],
      "metadata": {
        "id": "BDZuoOGzWUye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_v2(review_text):\n",
        "    return f\"\"\"\n",
        "You are a classification system.\n",
        "\n",
        "Your task:\n",
        "- Predict the Yelp star rating from 1 to 5 for the given review.\n",
        "\n",
        "STRICT RULES:\n",
        "- Respond with ONLY valid JSON.\n",
        "- Do NOT include markdown.\n",
        "- Do NOT include explanations outside JSON.\n",
        "- Do NOT include any extra text.\n",
        "\n",
        "JSON SCHEMA (follow exactly):\n",
        "{{\n",
        "  \"predicted_stars\": <integer from 1 to 5>,\n",
        "  \"explanation\": \"<one short sentence>\"\n",
        "}}\n",
        "\n",
        "Review:\n",
        "{review_text}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xO3dTu4xVrl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_v2 = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    prompt = prompt_v2(row[\"review_text\"])\n",
        "    output = call_llm(prompt)\n",
        "    parsed = parse_llm_output(output)\n",
        "\n",
        "    results_v2.append({\n",
        "        \"actual_stars\": row[\"stars\"],\n",
        "        \"predicted_stars\": parsed[\"predicted_stars\"] if parsed else None,\n",
        "        \"valid_json\": parsed is not None\n",
        "    })\n",
        "\n",
        "    time.sleep(1.5)\n"
      ],
      "metadata": {
        "id": "LKsP2w8cWhpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_v2 = pd.DataFrame(results_v2)\n",
        "res_v2.head()"
      ],
      "metadata": {
        "id": "tjLrBqKEWkS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt v2 Observations\n",
        "\n",
        "Although output structure was more constrained, prediction accuracy decreased.\n",
        "This highlights the trade-off between structure enforcement and reasoning quality."
      ],
      "metadata": {
        "id": "4KAdH3jvw5uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_v2 = (res_v2[\"actual_stars\"] == res_v2[\"predicted_stars\"]).mean()\n",
        "json_validity_v2 = res_v2[\"valid_json\"].mean()\n",
        "\n",
        "accuracy_v2, json_validity_v2"
      ],
      "metadata": {
        "id": "qogpnxC0hJnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Version 3 — Reasoned and Structured Prompt\n",
        "\n",
        "This prompt embeds reasoning within the JSON output to balance\n",
        "prediction quality and structured reliability."
      ],
      "metadata": {
        "id": "BfTdY-QLh0Zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_v3(review_text):\n",
        "    return f\"\"\"\n",
        "You are an expert sentiment analyst.\n",
        "\n",
        "Step 1: Analyze the review and identify:\n",
        "- overall sentiment (positive / neutral / negative)\n",
        "- key positive points\n",
        "- key negative points\n",
        "\n",
        "Step 2: Based on this analysis, decide the most appropriate Yelp star rating (1 to 5).\n",
        "\n",
        "STRICT OUTPUT RULES:\n",
        "- Your final answer MUST be valid JSON only.\n",
        "- Do NOT include markdown.\n",
        "- Do NOT include analysis text outside JSON.\n",
        "\n",
        "JSON FORMAT (follow exactly):\n",
        "{{\n",
        "  \"predicted_stars\": <integer from 1 to 5>,\n",
        "  \"explanation\": \"<brief justification based on positives and negatives>\"\n",
        "}}\n",
        "\n",
        "Review:\n",
        "{review_text}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LrUIBxIVhOT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_v3 = []\n",
        "for _, row in df.iterrows():\n",
        "    prompt = prompt_v3(row[\"review_text\"])\n",
        "    output = call_llm(prompt)\n",
        "    parsed = parse_llm_output(output)\n",
        "\n",
        "    results_v3.append({\n",
        "        \"actual_stars\": row[\"stars\"],\n",
        "        \"predicted_stars\": parsed[\"predicted_stars\"] if parsed else None,\n",
        "        \"valid_json\": parsed is not None\n",
        "    })\n",
        "\n",
        "    time.sleep(1.5)"
      ],
      "metadata": {
        "id": "7ytVT0E9jGif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execution Note\n",
        "\n",
        "Full execution of Prompt v3 was constrained by free-tier API rate limits.\n",
        "Repeated rate-limit responses were observed despite retry and backoff logic.\n",
        "\n",
        "The prompt design is retained for completeness and comparison.\n"
      ],
      "metadata": {
        "id": "1Oekl8-ixKzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res_v3 = pd.DataFrame(results_v3)\n",
        "res_v3.head()"
      ],
      "metadata": {
        "id": "iLG1nhvZjJjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_v3 = pd.DataFrame(results_v3)\n",
        "accuracy_v3 = (res_v3[\"actual_stars\"] == res_v3[\"predicted_stars\"]).mean()\n",
        "json_validity_v3 = res_v3[\"valid_json\"].mean()\n",
        "accuracy_v3, json_validity_v3"
      ],
      "metadata": {
        "id": "9IuB2yOiyFOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparative Summary\n",
        "\n",
        "| Prompt | Accuracy | JSON Validity | Key Observation |\n",
        "|-------|----------|---------------|----------------|\n",
        "| v1 | 0.175 | 0.30 | Weak structure |\n",
        "| v2 | 0.125 | 0.225 | Structure reduced reasoning |\n",
        "| v3 | N/A | Expected highest | Balanced design, quota-limited |\n"
      ],
      "metadata": {
        "id": "d1lfieF5xSWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_v3 = (res_v3[\"actual_stars\"] == res_v3[\"predicted_stars\"]).mean()\n",
        "json_validity_v3 = res_v3[\"valid_json\"].mean()\n",
        "\n",
        "accuracy_v3, json_validity_v3"
      ],
      "metadata": {
        "id": "TZEQBmMapkzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Takeaways\n",
        "\n",
        "- Prompt design strongly influences both accuracy and reliability\n",
        "- Strict structure alone does not guarantee better performance\n",
        "- Reasoning must be explicitly structured for automated pipelines\n",
        "- External constraints such as API limits are a real-world consideration"
      ],
      "metadata": {
        "id": "4fb_9zNIxRK6"
      }
    }
  ]
}